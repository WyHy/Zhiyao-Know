"""
ä»å…ƒæ•°æ®æ–‡ä»¶ç›´æ¥åŒæ­¥æ–‡ä»¶åˆ° kb_files è¡¨ï¼ˆæ›´ç®€å•çš„æ–¹æ³•ï¼‰

è¿è¡Œï¼šdocker compose exec api python scripts/sync_files_from_metadata.py
"""

import asyncio
import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.repositories.knowledge_file_repository import KnowledgeFileRepository
from src.storage.db.models import KBFile
from src.storage.postgres.manager import PostgresManager
from src.utils import logger


async def sync_from_metadata():
    """ä»å…ƒæ•°æ® JSON æ–‡ä»¶åŒæ­¥"""
    logger.info("=" * 70)
    logger.info("ä»å…ƒæ•°æ®æ–‡ä»¶åŒæ­¥æ–‡ä»¶åˆ° kb_files è¡¨")
    logger.info("=" * 70)
    
    # åˆå§‹åŒ–
    pg_manager = PostgresManager()
    pg_manager.initialize()
    kb_file_repo = KnowledgeFileRepository()
    
    # è¯»å– Milvus å…ƒæ•°æ®
    metadata_file = Path("saves/knowledge_base_data/milvus_data/metadata_milvus.json")
    if not metadata_file.exists():
        logger.warning("å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return
    
    with open(metadata_file) as f:
        metadata = json.load(f)
    
    files_meta = metadata.get("files", {})
    logger.info(f"\nğŸ“„ å…ƒæ•°æ®ä¸­æ‰¾åˆ° {len(files_meta)} ä¸ªæ–‡ä»¶")
    
    total_synced = 0
    total_skipped = 0
    total_errors = 0
    
    for file_id, file_meta in files_meta.items():
        try:
            kb_id = file_meta.get("database_id") or file_meta.get("db_id")
            if not kb_id:
                logger.warning(f"  âš ï¸  {file_meta.get('filename', file_id)}: ç¼ºå°‘ database_id")
                total_errors += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = await kb_file_repo.get_by_file_id(file_id)
            
            if existing:
                total_skipped += 1
                continue
            
            # ä½¿ç”¨ upsert åˆ›å»ºæˆ–æ›´æ–°ï¼ˆä¸ä¼ æ—¶é—´æˆ³ï¼Œè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆï¼‰
            data = {
                "db_id": kb_id,
                "filename": file_meta.get("filename", ""),
                "path": file_meta.get("path", ""),
                "file_size": file_meta.get("size", 0),
                "file_type": file_meta.get("file_type", ""),
                "status": file_meta.get("status", "indexed"),
            }
            
            await kb_file_repo.upsert(file_id, data)
            logger.info(f"  âœ… {file_meta.get('filename', file_id)} -> {kb_id}")
            total_synced += 1
            
        except Exception as e:
            logger.error(f"  âŒ {file_meta.get('filename', file_id)}: {e}")
            total_errors += 1
    
    # ç»Ÿè®¡
    logger.info(f"\n" + "=" * 70)
    logger.info(f"åŒæ­¥å®Œæˆï¼")
    logger.info(f"  âœ… æ–°å¢: {total_synced} ä¸ªæ–‡ä»¶")
    logger.info(f"  â­ï¸  è·³è¿‡: {total_skipped} ä¸ªæ–‡ä»¶ï¼ˆå·²å­˜åœ¨ï¼‰")
    logger.info(f"  âŒ é”™è¯¯: {total_errors} ä¸ª")
    logger.info("=" * 70)


async def main():
    try:
        await sync_from_metadata()
    except Exception as e:
        logger.error(f"\nâŒ åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())
