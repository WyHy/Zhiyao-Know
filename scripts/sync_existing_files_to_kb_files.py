"""
åŒæ­¥å·²å­˜åœ¨çš„æ–‡ä»¶åˆ° kb_files è¡¨

é—®é¢˜ï¼šä¹‹å‰ä¸Šä¼ çš„æ–‡ä»¶åªå­˜åœ¨äºçŸ¥è¯†åº“å…ƒæ•°æ®ä¸­ï¼Œæ²¡æœ‰å†™å…¥ kb_files è¡¨
è§£å†³ï¼šä»å„ä¸ªçŸ¥è¯†åº“çš„å…ƒæ•°æ®ä¸­è¯»å–æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒæ­¥åˆ° kb_files è¡¨

è¿è¡Œï¼šdocker compose exec api python scripts/sync_existing_files_to_kb_files.py
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src import knowledge_base
from src.repositories.knowledge_file_repository import KnowledgeFileRepository
from src.storage.db.models import KBFile
from src.utils import logger


async def sync_files():
    """åŒæ­¥æ‰€æœ‰çŸ¥è¯†åº“çš„æ–‡ä»¶åˆ° kb_files è¡¨"""
    logger.info("=" * 70)
    logger.info("å¼€å§‹åŒæ­¥çŸ¥è¯†åº“æ–‡ä»¶åˆ° kb_files è¡¨")
    logger.info("=" * 70)
    
    # åˆå§‹åŒ– PostgreSQL
    from src.storage.postgres.manager import PostgresManager
    pg_manager = PostgresManager()
    pg_manager.initialize()
    
    kb_file_repo = KnowledgeFileRepository()
    
    # ç›´æ¥ä»æ•°æ®åº“è·å–æ‰€æœ‰çŸ¥è¯†åº“
    from sqlalchemy import text
    async with pg_manager.get_async_session_context() as session:
        result = await session.execute(text("""
            SELECT db_id, name, kb_type
            FROM knowledge_bases
            ORDER BY created_at
        """))
        all_databases = [{"db_id": row[0], "name": row[1], "kb_type": row[2]} for row in result.fetchall()]
    
    logger.info(f"\nğŸ“š æ‰¾åˆ° {len(all_databases)} ä¸ªçŸ¥è¯†åº“")
    
    total_synced = 0
    total_skipped = 0
    total_errors = 0
    
    for db in all_databases:
        db_id = db.get("db_id")
        db_name = db.get("name", "æœªçŸ¥")
        kb_type = db.get("kb_type", "milvus")
        
        logger.info(f"\nå¤„ç†çŸ¥è¯†åº“: {db_name} ({db_id})")
        
        try:
            # æ ¹æ®ç±»å‹è·å–çŸ¥è¯†åº“å®ä¾‹
            if kb_type == "lightrag":
                from src.knowledge.implementations.lightrag import LightRAGKB
                kb_instance = LightRAGKB(work_dir="saves/knowledge_base_data/lightrag_data")
            else:  # milvus
                from src.knowledge.implementations.milvus import MilvusKB
                kb_instance = MilvusKB(work_dir="saves/knowledge_base_data/milvus_data")
            
            # ä»å…ƒæ•°æ®æ–‡ä»¶ä¸­è·å–æ–‡ä»¶åˆ—è¡¨
            if not hasattr(kb_instance, 'files_meta'):
                logger.warning(f"  âš ï¸  æ— æ³•è®¿é—® files_metaï¼Œè·³è¿‡")
                continue
            
            # ç­›é€‰è¯¥çŸ¥è¯†åº“çš„æ–‡ä»¶
            db_files = {
                fid: fmeta for fid, fmeta in kb_instance.files_meta.items()
                if fmeta.get("db_id") == db_id
            }
            
            if not db_files:
                logger.info(f"  â„¹ï¸  æ²¡æœ‰æ–‡ä»¶")
                continue
            
            logger.info(f"  ğŸ“„ æ‰¾åˆ° {len(db_files)} ä¸ªæ–‡ä»¶")
            
            for file_id, file_meta in db_files.items():
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = await kb_file_repo.get_by_id(file_id)
                    
                    if existing:
                        logger.debug(f"    - {file_meta.get('filename', 'unknown')} (å·²å­˜åœ¨ï¼Œè·³è¿‡)")
                        total_skipped += 1
                        continue
                    
                    # åˆ›å»ºæ–°è®°å½•
                    kb_file = KBFile(
                        file_id=file_id,
                        kb_id=db_id,
                        filename=file_meta.get("filename", ""),
                        file_path=file_meta.get("path", ""),
                        file_size=file_meta.get("size", 0),
                        file_type=file_meta.get("type", ""),
                        status=file_meta.get("status", "indexed"),
                        title=file_meta.get("title"),
                        summary=file_meta.get("summary"),
                        tags=file_meta.get("tags", []),
                        created_at=file_meta.get("created_at"),
                        updated_at=file_meta.get("updated_at"),
                        created_by=file_meta.get("created_by"),
                    )
                    
                    await kb_file_repo.create(kb_file)
                    logger.info(f"    âœ… {file_meta.get('filename', 'unknown')}")
                    total_synced += 1
                    
                except Exception as e:
                    logger.error(f"    âŒ {file_meta.get('filename', 'unknown')}: {e}")
                    total_errors += 1
                    
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†çŸ¥è¯†åº“å¤±è´¥: {e}")
            total_errors += 1
    
    # ç»Ÿè®¡
    logger.info(f"\n" + "=" * 70)
    logger.info(f"åŒæ­¥å®Œæˆï¼")
    logger.info(f"  âœ… æ–°å¢: {total_synced} ä¸ªæ–‡ä»¶")
    logger.info(f"  â­ï¸  è·³è¿‡: {total_skipped} ä¸ªæ–‡ä»¶ï¼ˆå·²å­˜åœ¨ï¼‰")
    logger.info(f"  âŒ é”™è¯¯: {total_errors} ä¸ª")
    logger.info("=" * 70)


async def main():
    try:
        await sync_files()
    except Exception as e:
        logger.error(f"\nâŒ åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())
